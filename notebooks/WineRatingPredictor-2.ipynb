{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Metric-Establish Baseline-Model Selection-Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will perform steps 4 and 5:\n",
    "\n",
    "4. Set Evaluation Metric & Establish Baseline\n",
    "5. Model Selection & Tune Hyperparameters of the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manipulation libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "# to display visuals in the notebook\n",
    "\n",
    "%config InlineBackend.figure_format='retina'\n",
    "#to enable high resolution plots\n",
    "\n",
    "# normalization and random-search and error metric\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# potential machine Learning Models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "import lightgbm as lgb\n",
    "\n",
    "# to save machine Learning Models\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to use in the notebook\n",
    "def fit_evaluate_model(model, X_train, y_train, \n",
    "                       X_valid, y_valid):\n",
    "    # function to train a given model\n",
    "    # return mean squared error of the\n",
    "    # actuals and predictions\n",
    "    model.fit(X_train, y_train)\n",
    "    y_predicted = model.predict(X_valid)\n",
    "    return mean_squared_error(y_valid, y_predicted)\n",
    "\n",
    "def convert_features_to_array(features):\n",
    "    # function to convert feature df\n",
    "    # to an array\n",
    "    num_rows = len(features)\n",
    "    num_cols = len(features.columns)\n",
    "    \n",
    "    features_array = (np\n",
    "                      .array(features)\n",
    "                      .reshape((num_rows, \n",
    "                                num_cols)))\n",
    "\n",
    "    return features_array\n",
    "\n",
    "def convert_target_to_array(target):\n",
    "    # function to convert target df\n",
    "    # to an array\n",
    "    target_array = (np\n",
    "                    .array(target)\n",
    "                    .reshape((-1, )))\n",
    "    return target_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and convert to array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"../transformed/X_train.csv\")\n",
    "y_train = pd.read_csv(\"../transformed/y_train.csv\")\n",
    "\n",
    "X_valid = pd.read_csv(\"../transformed/X_valid.csv\")\n",
    "y_valid = pd.read_csv(\"../transformed/y_valid.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_array = convert_target_to_array(y_train)\n",
    "y_valid_array = convert_target_to_array(y_valid)\n",
    "\n",
    "X_train_array = convert_features_to_array(X_train)\n",
    "X_valid_array = convert_features_to_array(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Evaluation Metric & Establish Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In real-world applications of data science/machine learning, the evaluation metric is set by data scientists in line with the stakeholder’s expectations from the ML model. That is why this is an important step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Mean square error (MSE)*** is the average of sum of squared residuals where a ***residual*** is the a difference between the actual and predicted value of a target variable. In other words, we are going to evaluate our model by looking at the measure of how large our squared errors (residuals) are spread out.\n",
    "\n",
    "Mean square error is selected as an error metric, because it is interpretable, it is analogous to variance and it also aligns with our selected algorithm's error minimization criteria.  \n",
    "\n",
    "On the other hand, this error metric is sensitive to extreme values or outliers. Since it takes the square of the differences between the actuals and predictions, in the presence of extreme values and outliers difference grows quadratically.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Establish Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before building moving to the model selection we are going to construct a common-sense baseline with MSE. A ***common-sense baseline*** can be explained as generating a naive guess of the target value by using some expert knowledge or few lines of code.\n",
    "\n",
    "We are going to select and tune the models that beats this baseline at a significant level. If any of them cannot beat it then machine learning may not be the best approach to solve this problem or whole preprocessing steps needs re-consideration. For our regression problem, a simple common-sense baseline is to predict the variance of the mean of the training set to the validation set. This approach aligns with our evaluation metric MSE as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88.45000745045448"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set baseline as mean of training set's target value\n",
    "baseline = (np\n",
    "            .mean(\n",
    "                y_train_array))\n",
    "baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline error is: 9.01\n"
     ]
    }
   ],
   "source": [
    "# calculate MSE baseline\n",
    "mse_baseline = (np\n",
    "                .mean(\n",
    "                    np.square(\n",
    "                        baseline - y_valid_array)))\n",
    "\n",
    "print(\"Baseline error is:\", round(mse_baseline, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that on average variance between the training points and validation points are 9.01. In other words, sum of squared residuals of average of training points to the validation points is 9.01."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select an ML Model Based on the Evaluation Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to try one linear algorithm, two distance-based algorithm and three tree-based algorithms:\n",
    "* Linear regression\n",
    "* K-nearest neighbors\n",
    "* Support vector machines\n",
    "* Random forests regressor\n",
    "* Extra trees regressor\n",
    "* Light gradient boostin machines\n",
    "\n",
    "We are going to train them with the training datasets, and compare their performances by looking at the MSE of the predictions of the validation set.\n",
    "\n",
    "For now, we will not go into the details of each model, we will observe the run time and the mean squared error reported by each model. We will elaborate on the best model in the last notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regresssion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of linear regression: 7.271003914920919\n",
      "CPU times: user 55 ms, sys: 93 ms, total: 148 ms\n",
      "Wall time: 26.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# linear regression\n",
    "lr = LinearRegression()\n",
    "mse_lr=fit_evaluate_model(lr, \n",
    "                          X_train_array, \n",
    "                          y_train_array, \n",
    "                          X_valid_array, \n",
    "                          y_valid_array)\n",
    "\n",
    "print(\"MSE of linear regression:\", mse_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression slightly decreased the baseline metric, showing that it is not a candidate to be a good predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize Datasets for KNN and SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distance-based models use the euclidian distance to train the models, thus varying ranges causes distance-based models to generate inaccurate predictions. In order to try different distance-based models, we are going to scale down the features with normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# apply normalization to training set and transform training set\n",
    "X_train_array_scaled = (scaler\n",
    "                        .fit_transform(\n",
    "                            X_train_array, y_train_array))\n",
    "\n",
    "# transform validation set\n",
    "X_valid_array_scaled = scaler.transform(X_valid_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Nearest Neighbors Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of knn regressor: 6.374447921323201\n",
      "CPU times: user 105 ms, sys: 18.9 ms, total: 124 ms\n",
      "Wall time: 130 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "knn = KNeighborsRegressor()\n",
    "mse_knn = fit_evaluate_model(knn,\n",
    "                             X_train_array_scaled, \n",
    "                             y_train_array, \n",
    "                             X_valid_array_scaled, \n",
    "                             y_valid_array)\n",
    "\n",
    "print(\"MSE of knn regressor:\", mse_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-nearest regressor performed better than the linear regression however, MSE is still high, showing that this algorithm is not a good predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of Support Vector Machines: 5.860241966386057\n",
      "CPU times: user 1.59 s, sys: 10.8 ms, total: 1.6 s\n",
      "Wall time: 1.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "svm = SVR()\n",
    "mse_svm = fit_evaluate_model(svm,\n",
    "                             X_train_array_scaled, \n",
    "                             y_train_array, \n",
    "                             X_valid_array_scaled, \n",
    "                             y_valid_array)\n",
    "\n",
    "print('MSE of Support Vector Machines:', mse_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support vector machines performed better than the k-nearest neighbors at a higher run-time. All in all MSE decreased 35% showing that this algorithm might be a candidate for a building a good predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of Random Forests 5.405698983941461\n",
      "CPU times: user 1.91 s, sys: 39.9 ms, total: 1.95 s\n",
      "Wall time: 1.95 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "mse_rf = fit_evaluate_model(rf,\n",
    "                            X_train_array,\n",
    "                            y_train_array,\n",
    "                            X_valid_array,\n",
    "                            y_valid_array)\n",
    "\n",
    "print(\"MSE of Random Forests\", mse_rf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
